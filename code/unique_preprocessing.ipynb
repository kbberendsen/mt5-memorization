{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                             prompt  \\\n",
      "0           0  De Nederlandse identiteit in boeken: land van ...   \n",
      "1           1  De Nederlandse identiteit in boeken: land van ...   \n",
      "2           2  De Nederlandse identiteit in boeken: land van ...   \n",
      "3           3  De Nederlandse identiteit in boeken: land van ...   \n",
      "4           4  De Nederlandse identiteit in boeken: land van ...   \n",
      "\n",
      "                                                 end  \\\n",
      "0  Oranje, die zich in het Wilhelmus niet als reb...   \n",
      "1  kunnen - de bewerkte heruitgave van een eerder...   \n",
      "2  Vier boeken zoeken het antwoord.\\nThomas von d...   \n",
      "3  . Zijn In het land der jaknikkers, een bundeli...   \n",
      "4  genoemde Volkskrantlezer, 'zeuren en zaniken',...   \n",
      "\n",
      "                                              result  lcs      lcsp  text_id  \\\n",
      "0  Het artikel bespreekt vier boeken die de Neder...   15  0.004069        0   \n",
      "1  de Nederlandse identiteit probeert te duiden, ...   30  0.004505        0   \n",
      "2  De verkiezingen gaan over de toekomst van Nede...   20  0.002818        0   \n",
      "3  In dit artikel bespreekt Thomas von der Dunk v...   13  0.004215        0   \n",
      "4  Nederlandse nuchterheid, 'Het land van de midd...   17  0.002956        0   \n",
      "\n",
      "   stop  \n",
      "0  stop  \n",
      "1  stop  \n",
      "2  stop  \n",
      "3  stop  \n",
      "4  stop  \n",
      "Index(['Unnamed: 0', 'prompt', 'end', 'result', 'lcs', 'lcsp', 'text_id',\n",
      "       'stop'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the unique data\n",
    "file_path = '..\\\\data\\\\unique\\\\volkskrant_gpt.csv'\n",
    "unique_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(unique_data.head())\n",
    "print(unique_data.columns)  # Print column names to verify structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  text_count\n",
      "0  Vier boeken zoeken antwoorden Over de toekomst...           1\n",
      "1  Vier boeken zoeken antwoorden Over de toekomst...           1\n",
      "2  Vier boeken zoeken antwoorden Over de toekomst...           1\n",
      "3  Vier boeken zoeken antwoorden Over de toekomst...           1\n",
      "4  Vier boeken zoeken antwoorden Over de toekomst...           1\n"
     ]
    }
   ],
   "source": [
    "# Function to clean and extract the first five sentences from the article\n",
    "def extract_first_sentences(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    if len(lines) >= 3:\n",
    "        # Combine lines after the first few to avoid grabbing the title/authors etc\n",
    "        combined_text = ' '.join(lines[1:])\n",
    "        # Split combined text into sentences using regex\n",
    "        sentences = re.split(r'[.!?]', combined_text)\n",
    "        # Join the first five sentences\n",
    "        first_sentences = '. '.join(sentences[:2])\n",
    "        return first_sentences\n",
    "    return ''\n",
    "\n",
    "# Process the first 2000 rows and extract the first few sentences from the article\n",
    "processed_data = []\n",
    "\n",
    "for index, row in unique_data.head(2000).iterrows():\n",
    "    prompt = row['prompt']\n",
    "    processed_prompt = extract_first_sentences(prompt)\n",
    "    processed_data.append({'text': processed_prompt})\n",
    "\n",
    "\n",
    "# Create a new DataFrame with the processed data\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "processed_df['text_count'] = 1\n",
    "\n",
    "# Display the processed DataFrame to verify\n",
    "print(processed_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  text_count\n",
      "0   Vier boeken zoeken antwoorden Over de toekomst...           1\n",
      "5   Populisten in Zweden beschuldigen sociaal-demo...           1\n",
      "10  Gratis busvervoer ontlokt op A44 geen daverend...           1\n",
      "15                                                              1\n",
      "17  Hayarpi Tamrazyan is al sinds 2010 met haar Ar...           1\n",
      "Number of unique processed prompts: 393\n"
     ]
    }
   ],
   "source": [
    "# Function to check if the first 5 words of two texts overlap\n",
    "def first_five_words_overlap(text1, text2):\n",
    "    words1 = text1.split()[:5]\n",
    "    words2 = text2.split()[:5]\n",
    "    return words1 == words2\n",
    "\n",
    "# Remove duplicates based on the first 5 overlapping words\n",
    "unique_processed_data = []\n",
    "for index, row in processed_df.iterrows():\n",
    "    is_duplicate = False\n",
    "    for unique_row in unique_processed_data:\n",
    "        if first_five_words_overlap(row['text'], unique_row['text']):\n",
    "            is_duplicate = True\n",
    "            break\n",
    "    if not is_duplicate:\n",
    "        unique_processed_data.append(row)\n",
    "\n",
    "# Create a new DataFrame with unique processed data\n",
    "unique_processed_df = pd.DataFrame(unique_processed_data)\n",
    "\n",
    "# Display the unique processed DataFrame\n",
    "print(unique_processed_df.head())\n",
    "print(f\"Number of unique processed prompts: {len(unique_processed_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  text_count  \\\n",
      "0   Vier boeken zoeken antwoorden Over de toekomst...           1   \n",
      "5   Populisten in Zweden beschuldigen sociaal-demo...           1   \n",
      "10  Gratis busvervoer ontlokt op A44 geen daverend...           1   \n",
      "15                                                              1   \n",
      "17  Hayarpi Tamrazyan is al sinds 2010 met haar Ar...           1   \n",
      "\n",
      "    Token Length  \n",
      "0             19  \n",
      "5             38  \n",
      "10            34  \n",
      "15             0  \n",
      "17            43  \n",
      "Average token length: 35.651399491094146\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "processed_df = unique_processed_df.drop_duplicates()\n",
    "processed_df = processed_df.dropna()\n",
    "\n",
    "# Calculate the token length for each prompt\n",
    "processed_df['Token Length'] = processed_df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate the average token length\n",
    "average_token_length = processed_df['Token Length'].mean()\n",
    "\n",
    "# Display the processed DataFrame and average token length\n",
    "print(processed_df.head())\n",
    "print(f\"Average token length: {average_token_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vier boeken zoeken antwoorden Over de toekomst van welk land gáán de verkiezingen eigenlijk.  Vier boeken zoeken het antwoord'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vier boeken zoeken antwoorden Over de toekomst...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Populisten in Zweden beschuldigen sociaal-demo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gratis busvervoer ontlokt op A44 geen daverend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hayarpi Tamrazyan is al sinds 2010 met haar Ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bellen metlandbouwverslaggever pieter hotse sm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>Roeien: Van Eupen soeverein naar halve finale ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>Guido van Woerkom, nu nog baas van de ANWB, is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>Generaal VS: kosten JSF moeten zakken De koste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>Redactie 12 januari 2016, 20:30 Duitsers en Pe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Overschot aan leraren basisschool Aankomende l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  text_count\n",
       "0     Vier boeken zoeken antwoorden Over de toekomst...           1\n",
       "5     Populisten in Zweden beschuldigen sociaal-demo...           1\n",
       "10    Gratis busvervoer ontlokt op A44 geen daverend...           1\n",
       "17    Hayarpi Tamrazyan is al sinds 2010 met haar Ar...           1\n",
       "20    bellen metlandbouwverslaggever pieter hotse sm...           1\n",
       "...                                                 ...         ...\n",
       "1973  Roeien: Van Eupen soeverein naar halve finale ...           1\n",
       "1978  Guido van Woerkom, nu nog baas van de ANWB, is...           1\n",
       "1983  Generaal VS: kosten JSF moeten zakken De koste...           1\n",
       "1993  Redactie 12 januari 2016, 20:30 Duitsers en Pe...           1\n",
       "1998  Overschot aan leraren basisschool Aankomende l...           1\n",
       "\n",
       "[363 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = unique_processed_df[['text', 'text_count']]\n",
    "df = df[df['text'].str.len() >= 100]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('..\\\\data\\\\unique\\\\text_unique_volkskrant.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
