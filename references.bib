@article{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      doi={10.48550/arXiv.1706.03762}
}

# Git repository
@software{berendsen2024unmasking_memorization,
      author={Berendsen, Kas},
      month=jun,
      title={{Unmasking Memorization: Assessing Dutch Language Memorization in mT5 Models}},
      year={2024},
      url={https://github.com/kbberendsen/ads-thesis}
}

# mT5
@article{xue2021mt5,
      title={mT5: A massively multilingual pre-trained text-to-text transformer}, 
      author={Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},
      year={2021},
      eprint={2010.11934},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      doi={10.48550/arXiv.2010.11934}
}

# Huggingface mT5 documentation
@misc{mT5,
      title={Documentation mT5 - Huggingface},
      url={https://huggingface.co/docs/transformers/model_doc/mt5},
      journal={mT5},
      publisher={Huggingface}
}

# T5 and introduction of (m)C4 dataset
@article{2019t5,
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
      journal={arXiv e-prints},
      year={2019},
      archivePrefix={arXiv},
      eprint={1910.10683},
      doi={10.48550/arXiv.1910.10683}
}

@article{nasr2023scalable,
      title={Scalable Extraction of Training Data from (Production) Language Models}, 
      author={Milad Nasr and Nicholas Carlini and Jonathan Hayase and Matthew Jagielski and A. Feder Cooper and Daphne Ippolito and Christopher A. Choquette-Choo and Eric Wallace and Florian Tramèr and Katherine Lee},
      year={2023},
      eprint={2311.17035},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      doi={10.48550/arXiv.2311.17035}
}

# Google paper
@article{carlini2023quantifying,
      title={Quantifying Memorization Across Neural Language Models}, 
      author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
      year={2023},
      eprint={2202.07646},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      doi={10.48550/arXiv.2202.07646}
}

# The Pile
@article{gao2020pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      doi={10.48550/arXiv.2101.00027}
}

@article{hartmann2023sok,
      title={SoK: Memorization in General-Purpose Large Language Models}, 
      author={Valentin Hartmann and Anshuman Suri and Vincent Bindschaedler and David Evans and Shruti Tople and Robert West},
      year={2023},
      eprint={2310.18362},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      doi={10.48550/arXiv.2310.18362}
}

@article{tirumala2022overfitting,
      author={Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
      booktitle={Advances in Neural Information Processing Systems},
      editor={S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
      pages={38274--38290},
      publisher={Curran Associates, Inc.},
      title={Memorization Without Overfitting:  Analyzing the Training Dynamics of Large Language Models},
      url={https://proceedings.neurips.cc/paper_files/paper/2022/file/fa0509f4dab6807e2cb465715bf2d249-Paper-Conference.pdf},
      volume={35},
      year={2022}
}

@misc{EuropeanParliament_2023,
      title={{EU AI Act}},
      author={{European Parliament}},
      url={https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence},
      journal={Topics | European Parliament},
      publisher={European Parliament},
      year={2023},
      month=dec
}

@article{naveed2024comprehensive,
      title={A Comprehensive Overview of Large Language Models}, 
      author={Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
      year={2024},
      eprint={2307.06435},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      doi={10.48550/arXiv.2307.06435}
}

@article{jiang2020xfactr,
    title = "{X}-{FACTR}: Multilingual Factual Knowledge Retrieval from Pretrained Language Models",
    author = "Jiang, Zhengbao  and
      Anastasopoulos, Antonios  and
      Araki, Jun  and
      Ding, Haibo  and
      Neubig, Graham",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.479",
    doi = "10.18653/v1/2020.emnlp-main.479",
    pages = "5943--5959",
    abstract = "Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as {``}Punta Cana is located in {\_}.{''} However, while knowledge is both written and queried in many languages, studies on LMs{'} factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at \url{https://x-factr.github.io}.",
}

@article{allenzhu2024physics,
      title={Physics of Language Models: Part 1, Learning Hierarchical Language Structures}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2024},
      eprint={2305.13673},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      doi={10.48550/arXiv.2305.13673}
}

# New York Times lawsuit
@misc{newyorktimes2023,
      title={{New York Times sues OpenAI and Microsoft over use of copyrighted work}}, 
      url={https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html}, 
      journal={The New York Times}, 
      author={Grynbaum, Michael and Mac, Ryan}, 
      year={2023}, 
      month=dec, 
      language={en} 
}

# Financial Times partnership
@misc{financialtimes2024,
      title={{Financial Times announces strategic partnership with OpenAI}}, 
      author={{The Financial Times}},
      url={https://aboutus.ft.com/press_release/openai},
      year={2024},
      month=apr
}

# Fine Google copyright vilation
@misc{finegoogle_2024, 
      title={{Google fined €250m in France for breaching intellectual property deal}}, 
      url={https://www.theguardian.com/technology/2024/mar/20/google-fined-250m-euros-in-france-for-breaching-intellectual-property-rules}, 
      journal={The Guardian}, 
      author={Angelique Chrisafis}, 
      year={2024}, 
      month=mar 
}

# Dutch copyright law
@misc{auteurswet_2022, 
      title={Auteurswet},
      url={https://wetten.overheid.nl/BWBR0001886/2022-10-01},
      year={2022},
      month=oct }